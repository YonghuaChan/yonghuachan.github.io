<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[kNN算法的DIY实现(仿照scikit-learn)]]></title>
    <url>%2F2019%2F08%2F15%2FDo-it-yourself-implementation-of-kNN-algorithm%2F</url>
    <content type="text"><![CDATA[什么是kNN算法?kNN算法, 又叫K最近邻算法, 可用于分类或者回归. 所谓K最近邻，就是k个最近的邻居的意思，说的是每个样本都可以用它最接近的k个邻居来代表。其特点是: 思想极度简单 应用数学知识几乎为零 效果好 可以解释机器学习算法使用过程中的很多细节问题 更完整刻画一起学习应用的流程 分类精度高, 对缺失值不敏感 懒加载, 模型复杂度高 kNN算法执行流程 根据欧几里得距离公式求出样本点与所有点的距离, 然后按照距离升序排序, 取出前k个点, 样本点是什么类别, 那么k个点就是什么类别 kNN的实质: k个样本如果足够地相似的话, 那么他们就很有可能属于同一个类别 自己动手实现kNN算法123456789101112131415161718192021222324import numpy as npfrom math import sqrtfrom collections import Counterdef kNN_classify(k, X_train, y_train, x): assert 1 &lt;= k &lt;= X_train.shape[0], "k must be valid" assert X_train.shape[0] == y_train.shape[0], \ "the size of X_train must equal to the size of y_train" assert X_train.shape[1] == x.shape[0], \ "the feature number of x must be equal to X_train" distances = [sqrt(np.sum((x_train - x)**2)) for x_train in X_train] nearest = np.argsort(distances) topK_y = [y_train[i] for i in nearest[:k]] votes = Counter(topK_y) return votes.most_common(1)[0][0]]]></content>
      <categories>
        <category>Machine Learning</category>
        <category>Algorithm</category>
        <category>Algorithm</category>
      </categories>
      <tags>
        <tag>knn</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[First Blog]]></title>
    <url>%2F2019%2F08%2F14%2Ffirst-blog%2F</url>
    <content type="text"><![CDATA[Start3W&amp;1H?你是谁? (Who)我是Yukirito, 一个兴趣使然的小朋友 为什么要做Blog? (Why)大约是3个月前, 我在尝试理解支持向量机的时候, 刷到了pluskid大神的Blog, 感到十分愉悦. Blog不像社交媒体, 输出碎片化信息. 而是类似于书记的子集. 按所含信息量多少来举例, 书籍&gt;Blog&gt;朋友圈, 所以Blog比较好输出较为结构化的信息. 这个Blog是搞什么的? (What)自己之前一直是在印象笔记上做笔记, 现在将逐步把闭门的数据选择后转移到Blog上来包括但不限于: 主业CS学习笔记 副业各种点歪了技能书的学习笔记 游戏攻略 个人心得 这个Blog是搞什么的? (What)emmmm还没开始更就有鸽的预感…原则上来说更新间隔不大于一周.]]></content>
      <categories>
        <category>MISC</category>
      </categories>
  </entry>
</search>
